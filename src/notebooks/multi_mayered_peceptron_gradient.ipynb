{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layered peceptron with gradient descent under Pytorch without using Pytorch maximize function for hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.X = torch.tensor(self.data.iloc[:, 1:].values, dtype=torch.float32)  # Les caractéristiques commencent à la colonne 1\n",
    "        # Normalisation\n",
    "        self.X /= 255\n",
    "        self.y = torch.tensor(self.data.iloc[:, 0].values, dtype=torch.long)     # La première colonne est la cible\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomDataset('../../data/sign_mnist_train.csv')\n",
    "testing_data = CustomDataset('../../data/sign_mnist_test.csv')\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True,)\n",
    "test_loader = DataLoader(testing_data, batch_size=32, shuffle=False,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init weights\n",
    "# 784 because there is 784 pixels in each image\n",
    "# 25 because there is 25 possible outputs : 0,1,2,3,4,5,6,7,8,9,...,25\n",
    "# Each pixel is linked to 25 outputs where each link is a weight to optimize\\\n",
    "# <=> Each class is linked to 784 pixel where each link is a weight to optimize\n",
    "weights = torch.randn(784, 25, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy on test set 0.04419966536530954\n"
     ]
    }
   ],
   "source": [
    "def test(weights, test_loader):\n",
    "    test_size = len(test_loader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        #print(batch_idx, data.shape, target.shape)\n",
    "        # data = data.view((-1, 28*28)) already done in the dataset\n",
    "        #print(batch_idx, data.shape, target.shape)\n",
    "\n",
    "        outputs = torch.matmul(data, weights)\n",
    "        softmax = F.softmax(outputs, dim=1)\n",
    "        pred = softmax.argmax(dim=1, keepdim=True)\n",
    "        n_correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        correct += n_correct\n",
    "\n",
    "    acc = correct / test_size\n",
    "    print(\" Accuracy on test set\", acc)\n",
    "    return\n",
    "\n",
    "test(weights, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shape: 5.177018642425537 Accuracy on test set 0.10345789180145008\n",
      "Loss shape: 5.5570988655090335 Accuracy on test set 0.13190184049079753\n",
      "Loss shape: 4.3229212760925298 Accuracy on test set 0.18558282208588958\n",
      "Loss shape: 3.5391714572906494 Accuracy on test set 0.19311210262130507\n",
      "Loss shape: 4.023074626922607 Accuracy on test set 0.2691020635805912\n",
      "Loss shape: 2.457159996032715 Accuracy on test set 0.30577244841048523\n",
      "Loss shape: 2.880488634109497 Accuracy on test set 0.3099553820412716\n",
      "Loss shape: 2.7475171089172363 Accuracy on test set 0.3414668153931958\n",
      "Loss shape: 2.7834656238555916 Accuracy on test set 0.3630786391522588\n",
      "Loss shape: 2.4642181396484375 Accuracy on test set 0.38162297824874514\n",
      "Loss shape: 2.2961087226867676 Accuracy on test set 0.37214166201896265\n",
      "Loss shape: 2.8648900985717773 Accuracy on test set 0.39598438371444505\n",
      "Loss shape: 2.2406008243560798 Accuracy on test set 0.4205242610150586\n",
      "Loss shape: 1.5588042736053467 Accuracy on test set 0.3877579475738985\n",
      "Loss shape: 1.4359697103500366 Accuracy on test set 0.4563580591187953\n",
      "Loss shape: 2.0782179832458496 Accuracy on test set 0.43572225320691577\n",
      "Loss shape: 1.9646545648574833 Accuracy on test set 0.435861684327942\n",
      "Loss shape: 1.7151418924331665 Accuracy on test set 0.4570552147239264\n",
      "Loss shape: 1.2704728841781616 Accuracy on test set 0.46193530395984383\n",
      "Loss shape: 1.6073020696640015 Accuracy on test set 0.4714166201896263\n",
      "Loss shape: 1.5568113327026367 Accuracy on test set 0.47532069157836027\n",
      "Loss shape: 1.1032017469406128 Accuracy on test set 0.4661182375906302\n",
      "Loss shape: 1.2776000499725342 Accuracy on test set 0.4774121583937535\n",
      "Loss shape: 1.4456635713577276 Accuracy on test set 0.4807585052983826\n",
      "Loss shape: 1.5163667201995853 Accuracy on test set 0.4977691020635806\n",
      "Loss shape: 1.3327668905258179 Accuracy on test set 0.4947016174010039\n",
      "Loss shape: 1.2014380693435674 Accuracy on test set 0.5002788622420524\n",
      "Loss shape: 1.5404427051544194 Accuracy on test set 0.4905186837702175\n",
      "Loss shape: 1.1955742835998535 Accuracy on test set 0.5175683212493029\n",
      "Loss shape: 0.6913989782333374 Accuracy on test set 0.5390407138873397\n",
      "Loss shape: 1.2631480693817139 Accuracy on test set 0.5149191299498048\n",
      "Loss shape: 1.4757415056228638 Accuracy on test set 0.5306748466257669\n",
      "Loss shape: 0.8829950690269476 Accuracy on test set 0.5310931399888456\n",
      "Loss shape: 1.6607117652893066 Accuracy on test set 0.5497769102063581\n",
      "Loss shape: 0.7568976879119873 Accuracy on test set 0.5398773006134969\n",
      "Loss shape: 1.4905154705047607 Accuracy on test set 0.4817345231455661\n",
      "Loss shape: 1.3245625495910645 Accuracy on test set 0.5291411042944786\n",
      "Loss shape: 0.89780932664871227 Accuracy on test set 0.5076687116564417\n",
      "Loss shape: 1.50871860980987556 Accuracy on test set 0.4841048522030117\n",
      "Loss shape: 1.03709220886230474 Accuracy on test set 0.5389012827663134\n",
      "Loss shape: 0.87919980287551883 Accuracy on test set 0.5193809258226436\n",
      "Loss shape: 1.12877047061920174 Accuracy on test set 0.531372002230898\n",
      "Loss shape: 0.80707925558090215 Accuracy on test set 0.5018126045733408\n",
      "Loss shape: 0.6574715971946716 Accuracy on test set 0.5554935861684328\n",
      "Loss shape: 0.95981854200363167 Accuracy on test set 0.4831288343558282\n",
      "Loss shape: 1.06319248676300055 Accuracy on test set 0.5596765197992192\n",
      "Loss shape: 1.07623720169067384 Accuracy on test set 0.5369492470719465\n",
      "Loss shape: 0.76443088054656985 Accuracy on test set 0.5436419408812047\n",
      "Loss shape: 0.79138273000717166 Accuracy on test set 0.5475460122699386\n",
      "Loss shape: 1.19759047031402596 Accuracy on test set 0.48870607919687675\n",
      "Loss shape: 0.70714092254638674 Accuracy on test set 0.5560513106525377\n",
      "Loss shape: 0.64043712615966895 Accuracy on test set 0.5462911321807027\n",
      "Loss shape: 0.59593939781188964 Accuracy on test set 0.5450362520914668\n",
      "Loss shape: 0.52172195911407476 Accuracy on test set 0.569994422755159\n",
      "Loss shape: 0.59546530246734623 Accuracy on test set 0.5789180145008366\n",
      "Loss shape: 0.68048286437988285 Accuracy on test set 0.5744562186279978\n",
      "Loss shape: 0.42490804195404056 Accuracy on test set 0.5851924149470161\n",
      "Loss shape: 0.70030277967453387 Accuracy on test set 0.5557724484104852\n",
      "Loss shape: 0.65505421161651614 Accuracy on test set 0.5914668153931958\n",
      "Loss shape: 0.78932124376297364 Accuracy on test set 0.5628834355828221\n",
      "Loss shape: 0.51883924007415775 Accuracy on test set 0.5786391522587842\n",
      "Loss shape: 0.71896386146545414 Accuracy on test set 0.5712493028443949\n",
      "Loss shape: 0.66451901197433475 Accuracy on test set 0.5690184049079755\n",
      "Loss shape: 0.74443316459655763 Accuracy on test set 0.5741773563859454\n",
      "Loss shape: 1.19121658802032476 Accuracy on test set 0.5522866703848299\n",
      "Loss shape: 0.50926160812377934 Accuracy on test set 0.553680981595092\n",
      "Loss shape: 0.67531257867813115 Accuracy on test set 0.5850529838259899\n",
      "Loss shape: 0.48610043525695894 Accuracy on test set 0.5787785833798104\n",
      "Loss shape: 0.63011628389358524 Accuracy on test set 0.6030395984383714\n",
      "Loss shape: 0.66622912883758548 Accuracy on test set 0.5692972671500279\n",
      "Loss shape: 0.56081044673919683 Accuracy on test set 0.5800334634690463\n",
      "Loss shape: 0.42320013046264657 Accuracy on test set 0.5683212493028444\n",
      "Loss shape: 0.71459054946899416 Accuracy on test set 0.5883993307306191\n",
      "Loss shape: 0.37807857990264894 Accuracy on test set 0.6016452872281093\n",
      "Loss shape: 0.39220878481864934 Accuracy on test set 0.5634411600669269\n",
      "Loss shape: 0.45195803046226547 Accuracy on test set 0.602760736196319\n",
      "Loss shape: 0.61788094043731695 Accuracy on test set 0.5744562186279978\n",
      "Loss shape: 0.61699545383453375 Accuracy on test set 0.5655326268823201\n",
      "Loss shape: 0.34338149428367615 Accuracy on test set 0.5832403792526492\n",
      "Loss shape: 0.54891550540924075 Accuracy on test set 0.5825432236475181\n",
      "Loss shape: 0.47024795413017273 Accuracy on test set 0.5943948689347462\n",
      "Loss shape: 0.21983346343040466 Accuracy on test set 0.5911879531511434\n",
      "Loss shape: 0.51223164796829225 Accuracy on test set 0.613218070273285\n",
      "Loss shape: 0.55402201414108285 Accuracy on test set 0.5980200780814278\n",
      "Loss shape: 0.58575332164764417 Accuracy on test set 0.5635805911879531\n",
      "Loss shape: 0.49717363715171814 Accuracy on test set 0.5928611266034579\n",
      "Loss shape: 0.55679792165756234 Accuracy on test set 0.6176798661461238\n",
      "Loss shape: 0.73037052154541026 Accuracy on test set 0.6210262130507529\n",
      "Loss shape: 0.38726806640625 Accuracy on test set 0.6119631901840491\n",
      "Loss shape: 0.62478417158126836 Accuracy on test set 0.6084774121583938\n",
      "Loss shape: 0.34333232045173645 Accuracy on test set 0.602760736196319\n",
      "Loss shape: 0.46816074848175056 Accuracy on test set 0.5999721137757947\n",
      "Loss shape: 0.41024768352508545 Accuracy on test set 0.6031790295593976\n",
      "Loss shape: 0.65756565332412727 Accuracy on test set 0.5836586726157278\n",
      "Loss shape: 0.25774487853050236 Accuracy on test set 0.5941160066926938\n",
      "Loss shape: 0.28913429379463196 Accuracy on test set 0.6070831009481317\n",
      "Loss shape: 0.4413931369781494 Accuracy on test set 0.5967651979921919\n",
      "Loss shape: 0.68997395038604743 Accuracy on test set 0.5991355270496375\n",
      "Loss shape: 0.58740460872650156 Accuracy on test set 0.5725041829336308\n",
      "Loss shape: 0.57670861482620245 Accuracy on test set 0.6278583379810374\n",
      "Loss shape: 0.19928763806819916 Accuracy on test set 0.6200501952035694\n",
      "Loss shape: 0.38188171386718754 Accuracy on test set 0.6193530395984383\n",
      "Loss shape: 0.50853526592254644 Accuracy on test set 0.6189347462353597\n",
      "Loss shape: 0.53454697132110657 Accuracy on test set 0.6013664249860569\n",
      "Loss shape: 0.29071110486984253 Accuracy on test set 0.6062465142219744\n",
      "Loss shape: 0.47292989492416385 Accuracy on test set 0.6253485778025656\n",
      "Loss shape: 0.28122019767761235 Accuracy on test set 0.6201896263245956\n",
      "Loss shape: 0.24569080770015717 Accuracy on test set 0.6070831009481317\n",
      "Loss shape: 0.33096849918365487 Accuracy on test set 0.6056887897378694\n",
      "Loss shape: 0.28451123833656313 Accuracy on test set 0.6094534300055773\n",
      "Loss shape: 0.23856028914451645 Accuracy on test set 0.6118237590630229\n",
      "Loss shape: 0.53914999961853034 Accuracy on test set 0.6183770217512549\n",
      "Loss shape: 0.5846923589706421 Accuracy on test set 0.6288343558282209\n",
      "Loss shape: 0.30157092213630676 Accuracy on test set 0.5877021751254881\n",
      "Loss shape: 0.33555677533149724 Accuracy on test set 0.6048522030117122\n",
      "Loss shape: 0.45169633626937866 Accuracy on test set 0.6091745677635249\n",
      "Loss shape: 0.58997243642807014 Accuracy on test set 0.6200501952035694\n",
      "Loss shape: 0.31350773572921753 Accuracy on test set 0.61781929726715\n",
      "Loss shape: 0.47901085019111633 Accuracy on test set 0.628276631344116\n",
      "Loss shape: 0.47815385460853577 Accuracy on test set 0.6073619631901841\n",
      "Loss shape: 0.15463811159133914 Accuracy on test set 0.6296709425543782\n",
      "Loss shape: 0.25482454895973206 Accuracy on test set 0.6210262130507529\n",
      "Loss shape: 0.32191684842109686 Accuracy on test set 0.6270217512548801\n",
      "Loss shape: 0.10002613067626953 Accuracy on test set 0.606664807585053\n",
      "Loss shape: 0.22217942774295807 Accuracy on test set 0.613218070273285\n",
      "Loss shape: 0.26119413971900942 Accuracy on test set 0.6016452872281093\n",
      "Loss shape: 0.34475535154342657 Accuracy on test set 0.6121026213050753\n",
      "Loss shape: 0.30311593413352966 Accuracy on test set 0.6111266034578918\n",
      "Loss shape: 0.42298889160156253 Accuracy on test set 0.634132738427217\n",
      "Loss shape: 0.37304091453552246 Accuracy on test set 0.5977412158393753\n",
      "Loss shape: 0.27545559406280527 Accuracy on test set 0.6101505856107083\n",
      "Loss shape: 0.40247964859008796 Accuracy on test set 0.6077802565532627\n",
      "Loss shape: 0.18026690185070038 Accuracy on test set 0.6104294478527608\n",
      "Loss shape: 0.32466500997543335 Accuracy on test set 0.6228388176240937\n",
      "Loss shape: 0.34709739685058594 Accuracy on test set 0.6146123814835471\n",
      "Loss shape: 0.37012735009193426 Accuracy on test set 0.6398494143892917\n",
      "Loss shape: 0.49841737747192383 Accuracy on test set 0.6093139988845511\n",
      "Loss shape: 0.22768075764179238 Accuracy on test set 0.6286949247071947\n",
      "Loss shape: 0.19744384288787842 Accuracy on test set 0.6160066926938093\n",
      "Loss shape: 0.21551443636417395 Accuracy on test set 0.6303680981595092\n",
      "Loss shape: 0.34292382001876836 Accuracy on test set 0.5882598996095929\n",
      "Loss shape: 0.39492493867874146 Accuracy on test set 0.6123814835471277\n",
      "Loss shape: 0.24321837723255157 Accuracy on test set 0.6383156720580033\n",
      "Loss shape: 0.29175749421119694 Accuracy on test set 0.6048522030117122\n",
      "Loss shape: 0.31354567408561707 Accuracy on test set 0.6252091466815394\n",
      "Loss shape: 0.52570831775665287 Accuracy on test set 0.6162855549358617\n",
      "Loss shape: 0.22428812086582184 Accuracy on test set 0.600808700501952\n",
      "Loss shape: 0.28182262182235723 Accuracy on test set 0.6003904071388734\n",
      "Loss shape: 0.23725022375583652 Accuracy on test set 0.6359453430005577\n",
      "Loss shape: 0.33625435829162684 Accuracy on test set 0.6337144450641383\n",
      "Loss shape: 0.26923614740371704 Accuracy on test set 0.6146123814835471\n",
      "Loss shape: 0.28710845112800614 Accuracy on test set 0.6182375906302287\n",
      "Loss shape: 0.14359916746616364 Accuracy on test set 0.6203290574456218\n",
      "Loss shape: 0.36424878239631653 Accuracy on test set 0.6249302844394868\n",
      "Loss shape: 0.28001970052719116 Accuracy on test set 0.6210262130507529\n",
      "Loss shape: 0.29401758313179016 Accuracy on test set 0.6185164528722811\n",
      "Loss shape: 0.18965043127536774 Accuracy on test set 0.6221416620189626\n",
      "Loss shape: 0.26092687249183655 Accuracy on test set 0.6080591187953152\n",
      "Loss shape: 0.36013734340667725 Accuracy on test set 0.625627440044618\n",
      "Loss shape: 0.25384581089019775 Accuracy on test set 0.6186558839933073\n",
      "Loss shape: 0.16930760443210602 Accuracy on test set 0.6164249860568879\n",
      "Loss shape: 0.22654072940349585 Accuracy on test set 0.6072225320691579\n",
      "Loss shape: 0.23025353252887726 Accuracy on test set 0.6247908533184606\n",
      "Loss shape: 0.29912781715393066 Accuracy on test set 0.630228667038483\n",
      "Loss shape: 0.38441649079322815 Accuracy on test set 0.6137757947573899\n",
      "Loss shape: 0.20718607306480408 Accuracy on test set 0.6094534300055773\n",
      "Loss shape: 0.22944575548171997 Accuracy on test set 0.6206079196876743\n",
      "Loss shape: 0.20890262722969055 Accuracy on test set 0.6242331288343558\n",
      "Loss shape: 0.31274160742759705 Accuracy on test set 0.6100111544896821\n",
      "Loss shape: 0.21729212999343872 Accuracy on test set 0.6408254322364751\n",
      "Loss shape: 0.18569216132164001 Accuracy on test set 0.6193530395984383\n",
      "Loss shape: 0.43439400196075444 Accuracy on test set 0.6141940881204685\n",
      "Loss shape: 0.36326211690902712 Accuracy on test set 0.6268823201338539\n",
      "Loss shape: 0.28889185190200806 Accuracy on test set 0.625627440044618\n",
      "Loss shape: 0.30251389741897583 Accuracy on test set 0.6320412716118238\n",
      "Loss shape: 0.24562022089958197 Accuracy on test set 0.6390128276631344\n",
      "Loss shape: 0.12824684381484985 Accuracy on test set 0.6199107640825432\n",
      "Loss shape: 0.22369438409805298 Accuracy on test set 0.6176798661461238\n",
      "Loss shape: 0.31243011355400085 Accuracy on test set 0.6316229782487451\n",
      "Loss shape: 0.27108311653137207 Accuracy on test set 0.6377579475738985\n",
      "Loss shape: 0.17164760828018188 Accuracy on test set 0.6268823201338539\n",
      "Loss shape: 0.26354959607124331 Accuracy on test set 0.6168432794199665\n",
      "Loss shape: 0.42149814963340765 Accuracy on test set 0.6148912437255996\n",
      "Loss shape: 0.09558010846376419 Accuracy on test set 0.6299498047964306\n",
      "Loss shape: 0.204387202858924875 Accuracy on test set 0.6402677077523703\n",
      "Loss shape: 0.21013183891773224 Accuracy on test set 0.6281372002230898\n",
      "Loss shape: 0.40168610215187077 Accuracy on test set 0.6076408254322365\n",
      "Loss shape: 0.32722619175910953 Accuracy on test set 0.6356664807585053\n",
      "Loss shape: 0.21047119796276093 Accuracy on test set 0.6320412716118238\n",
      "Loss shape: 0.18489697575569153 Accuracy on test set 0.6369213608477412\n",
      "Loss shape: 0.35433530807495117 Accuracy on test set 0.6305075292805354\n",
      "Loss shape: 0.19647070765495375 Accuracy on test set 0.636781929726715\n",
      "Loss shape: 0.143336087465286254 Accuracy on test set 0.6180981595092024\n",
      "Loss shape: 0.38927423954010016 Accuracy on test set 0.6229782487451199\n",
      "Loss shape: 0.24133670330047607 Accuracy on test set 0.6220022308979364\n",
      "Loss shape: 0.22419300675392157 Accuracy on test set 0.615867261572783\n",
      "Loss shape: 0.19694834947586063 Accuracy on test set 0.6180981595092024\n",
      "Loss shape: 0.21177823841571808 Accuracy on test set 0.6249302844394868\n",
      "Loss shape: 0.26869574189186096 Accuracy on test set 0.6266034578918015\n",
      "Loss shape: 0.27078515291213998 Accuracy on test set 0.634829894032348\n",
      "Loss shape: 0.31342449784278878 Accuracy on test set 0.6123814835471277\n",
      "Loss shape: 0.18962496519088745 Accuracy on test set 0.6242331288343558\n",
      "Loss shape: 0.20127679407596588 Accuracy on test set 0.6211656441717791\n",
      "Loss shape: 0.14064271748065948 Accuracy on test set 0.6372002230897936\n",
      "Loss shape: 0.23950751125812536 Accuracy on test set 0.6253485778025656\n",
      "Loss shape: 0.19449177384376526 Accuracy on test set 0.6293920803123257\n",
      "Loss shape: 0.19503703713417053 Accuracy on test set 0.6316229782487451\n",
      "Loss shape: 0.23107013106346137 Accuracy on test set 0.619771332961517\n",
      "Loss shape: 0.12147130817174911 Accuracy on test set 0.6116843279419967\n",
      "Loss shape: 0.29273372888565063 Accuracy on test set 0.630228667038483\n",
      "Loss shape: 0.20554439723491675 Accuracy on test set 0.6278583379810374\n",
      "Loss shape: 0.19313542544841766 Accuracy on test set 0.6384551031790295\n",
      "Loss shape: 0.16472212970256805 Accuracy on test set 0.6378973786949247\n",
      "Loss shape: 0.17487335205078125 Accuracy on test set 0.6390128276631344\n",
      "Loss shape: 0.15814533829689026 Accuracy on test set 0.64138315672058\n",
      "Loss shape: 0.13589155673980713 Accuracy on test set 0.6239542665923034\n",
      "Loss shape: 0.21186183393001556 Accuracy on test set 0.6107083100948132\n",
      "Loss shape: 0.18649911880493164 Accuracy on test set 0.6388733965421082\n",
      "Loss shape: 0.12096063047647476 Accuracy on test set 0.6185164528722811\n",
      "Loss shape: 0.250063180923461934 Accuracy on test set 0.6274400446179588\n",
      "Loss shape: 0.13623024523258214 Accuracy on test set 0.6257668711656442\n",
      "Loss shape: 0.22688628733158112 Accuracy on test set 0.621723368655884\n",
      "Loss shape: 0.12315677851438522 Accuracy on test set 0.6424986056887897\n",
      "Loss shape: 0.16421210765838623 Accuracy on test set 0.6253485778025656\n",
      "Loss shape: 0.15003153681755066 Accuracy on test set 0.6376185164528723\n",
      "Loss shape: 0.14271685481071472 Accuracy on test set 0.6240936977133296\n",
      "Loss shape: 0.253748059272766154 Accuracy on test set 0.6296709425543782\n",
      "Loss shape: 0.12616969645023346 Accuracy on test set 0.6331567205800335\n",
      "Loss shape: 0.38591608405113224 Accuracy on test set 0.6229782487451199\n",
      "Loss shape: 0.12444344907999039 Accuracy on test set 0.6437534857780256\n",
      "Loss shape: 0.18760897219181068 Accuracy on test set 0.6104294478527608\n",
      "Loss shape: 0.24712292850017548 Accuracy on test set 0.6247908533184606\n",
      "Loss shape: 0.11388780176639557 Accuracy on test set 0.6319018404907976\n",
      "Loss shape: 0.20930896699428558 Accuracy on test set 0.6260457334076966\n",
      "Loss shape: 0.21563132107257843 Accuracy on test set 0.6121026213050753\n",
      "Loss shape: 0.23131892085075378 Accuracy on test set 0.6299498047964306\n",
      "Loss shape: 0.1343533992767334 Accuracy on test set 0.6528165086447295\n",
      "Loss shape: 0.08796711266040802 Accuracy on test set 0.6312046848856665\n",
      "Loss shape: 0.18783456087112427 Accuracy on test set 0.6358059118795315\n",
      "Loss shape: 0.19059582054615027 Accuracy on test set 0.6489124372559956\n",
      "Loss shape: 0.11129038035869598 Accuracy on test set 0.6259063022866704\n",
      "Loss shape: 0.157824352383613684 Accuracy on test set 0.644032348020078\n",
      "Loss shape: 0.17176388204097748 Accuracy on test set 0.632877858337981\n",
      "Loss shape: 0.21145023405551913 Accuracy on test set 0.6268823201338539\n",
      "Loss shape: 0.26989382505416874 Accuracy on test set 0.64138315672058\n",
      "Loss shape: 0.09211727231740952 Accuracy on test set 0.6426380368098159\n",
      "Loss shape: 0.32337117195129395 Accuracy on test set 0.6303680981595092\n",
      "Loss shape: 0.17878670990467072 Accuracy on test set 0.6426380368098159\n",
      "Loss shape: 0.17613220214843756 Accuracy on test set 0.63218070273285\n",
      "Loss shape: 0.2308788150548935 Accuracy on test set 0.6397099832682654\n",
      "Loss shape: 0.18618904054164886 Accuracy on test set 0.6281372002230898\n",
      "Loss shape: 0.2194884866476059 Accuracy on test set 0.6356664807585053\n",
      "Loss shape: 0.21932086348533632 Accuracy on test set 0.6338538761851645\n",
      "Loss shape: 0.16588546335697174 Accuracy on test set 0.644032348020078\n",
      "Loss shape: 0.16808031499385834 Accuracy on test set 0.6309258226436141\n",
      "Loss shape: 0.12445930391550064 Accuracy on test set 0.6285554935861685\n",
      "Loss shape: 0.09890479594469076 Accuracy on test set 0.6482152816508645\n",
      "Loss shape: 0.14870670437812805 Accuracy on test set 0.6330172894590073\n",
      "Loss shape: 0.1027073860168457 Accuracy on test set 0.6319018404907976\n",
      "Loss shape: 0.22283920645713806 Accuracy on test set 0.6402677077523703\n",
      "Loss shape: 0.15724249184131622 Accuracy on test set 0.6370607919687674\n",
      "Loss shape: 0.11384598910808563 Accuracy on test set 0.634829894032348\n",
      "Loss shape: 0.13216465711593628 Accuracy on test set 0.6405465699944227\n",
      "Loss shape: 0.10301382839679718 Accuracy on test set 0.6337144450641383\n",
      "Loss shape: 0.21055227518081665 Accuracy on test set 0.6134969325153374\n",
      "Loss shape: 0.15952384471893312 Accuracy on test set 0.6293920803123257\n",
      "Loss shape: 0.161791041493415834 Accuracy on test set 0.629531511433352\n",
      "Loss shape: 0.19888162612915048 Accuracy on test set 0.6334355828220859\n",
      "Loss shape: 0.11813025176525116 Accuracy on test set 0.6299498047964306\n",
      "Loss shape: 0.12424108386039734 Accuracy on test set 0.6339933073061907\n",
      "Loss shape: 0.09002412110567093 Accuracy on test set 0.6208867819297267\n",
      "Loss shape: 0.22607576847076416 Accuracy on test set 0.6392916899051868\n",
      "Loss shape: 0.14321993291378022 Accuracy on test set 0.6476575571667597\n",
      "Loss shape: 0.1091768741607666 Accuracy on test set 0.6293920803123257\n",
      "Loss shape: 0.08762544393539429 Accuracy on test set 0.6334355828220859\n",
      "Loss shape: 0.09536854177713394 Accuracy on test set 0.6377579475738985\n",
      "Loss shape: 0.160312950611114566 Accuracy on test set 0.6399888455103179\n",
      "Loss shape: 0.194441512227058454 Accuracy on test set 0.6447295036252092\n",
      "Loss shape: 0.182220697402954185 Accuracy on test set 0.6323201338538762\n",
      "Loss shape: 0.131976842880249025 Accuracy on test set 0.6438929168990518\n",
      "Loss shape: 0.119513854384422315 Accuracy on test set 0.6342721695482432\n",
      "Loss shape: 0.161446690559387294 Accuracy on test set 0.6270217512548801\n",
      "Loss shape: 0.126031965017318735 Accuracy on test set 0.6383156720580033\n",
      "Loss shape: 0.17969280481338555 Accuracy on test set 0.6284160624651423\n",
      "Loss shape: 0.221291840076446534 Accuracy on test set 0.6378973786949247\n",
      "Loss shape: 0.07917603850364685 Accuracy on test set 0.622420524261015\n",
      "Loss shape: 0.20484541356563568 Accuracy on test set 0.6366424986056888\n",
      "Loss shape: 0.07330006361007693 Accuracy on test set 0.6316229782487451\n",
      "Loss shape: 0.12424274533987045 Accuracy on test set 0.6376185164528723\n",
      "Loss shape: 0.10472771525382996 Accuracy on test set 0.6263245956497491\n",
      "Loss shape: 0.12303884327411652 Accuracy on test set 0.6360847741215839\n",
      "Loss shape: 0.15112555027008057 Accuracy on test set 0.6390128276631344\n",
      "Loss shape: 0.21392208337783813 Accuracy on test set 0.6285554935861685\n",
      "Loss shape: 0.17458359897136688 Accuracy on test set 0.6384551031790295\n",
      "Loss shape: 0.10450595617294312 Accuracy on test set 0.6390128276631344\n",
      "Loss shape: 0.151434704661369326 Accuracy on test set 0.6457055214723927\n",
      "Loss shape: 0.13144280016422272 Accuracy on test set 0.6423591745677635\n",
      "Loss shape: 0.074899457395076755 Accuracy on test set 0.6447295036252092\n",
      "Loss shape: 0.12524248659610748 Accuracy on test set 0.6390128276631344\n",
      "Loss shape: 0.167285144329071046 Accuracy on test set 0.6320412716118238\n",
      "Loss shape: 0.10338434576988228 Accuracy on test set 0.644590072504183\n",
      "Loss shape: 0.109928131103515625 Accuracy on test set 0.6363636363636364\n",
      "Loss shape: 0.16175042092800147 Accuracy on test set 0.6406860011154489\n",
      "Loss shape: 0.09504742920398712 Accuracy on test set 0.6420803123257111\n",
      "Loss shape: 0.123133748769760136 Accuracy on test set 0.6299498047964306\n",
      "Loss shape: 0.090982012450695044 Accuracy on test set 0.6292526491912995\n",
      "Loss shape: 0.115664839744567876 Accuracy on test set 0.6369213608477412\n",
      "Loss shape: 0.058461301028728485 Accuracy on test set 0.6457055214723927\n",
      "Loss shape: 0.170991659164428775 Accuracy on test set 0.6317624093697713\n",
      "Loss shape: 0.092780046164989475 Accuracy on test set 0.6310652537646403\n",
      "Loss shape: 0.103306949138641364 Accuracy on test set 0.624372559955382\n",
      "Loss shape: 0.11727982014417648 Accuracy on test set 0.6316229782487451\n",
      "Loss shape: 0.228157594799995425 Accuracy on test set 0.6288343558282209\n",
      "Loss shape: 0.137179031968116765 Accuracy on test set 0.6356664807585053\n",
      "Loss shape: 0.176475062966346745 Accuracy on test set 0.645287228109314\n",
      "Loss shape: 0.08020669966936111 Accuracy on test set 0.6305075292805354\n",
      "Loss shape: 0.08466494828462601 Accuracy on test set 0.6366424986056888\n",
      "Loss shape: 0.215420216321945214 Accuracy on test set 0.6430563301728945\n",
      "Loss shape: 0.07487257570028305 Accuracy on test set 0.6391522587841606\n",
      "Loss shape: 0.15861402451992035 Accuracy on test set 0.6420803123257111\n",
      "Loss shape: 0.089276596903800966 Accuracy on test set 0.6365030674846626\n",
      "Loss shape: 0.12449143826961517 Accuracy on test set 0.6391522587841606\n",
      "Loss shape: 0.165087461471557626 Accuracy on test set 0.6345510317902956\n",
      "Loss shape: 0.13247227668762207 Accuracy on test set 0.6376185164528723\n",
      "Loss shape: 0.11624660342931747 Accuracy on test set 0.6422197434467373\n",
      "Loss shape: 0.108053043484687854 Accuracy on test set 0.6383156720580033\n",
      "Loss shape: 0.13030165433883667 Accuracy on test set 0.6239542665923034\n",
      "Loss shape: 0.14724539220333128 Accuracy on test set 0.6267428890128277\n",
      "Loss shape: 0.106310606002807626 Accuracy on test set 0.6436140546569994\n",
      "Loss shape: 0.130860984325408944 Accuracy on test set 0.6345510317902956\n",
      "Loss shape: 0.055046897381544115 Accuracy on test set 0.6438929168990518\n",
      "Loss shape: 0.08291087299585342 Accuracy on test set 0.6298103736754044\n",
      "Loss shape: 0.169896066188812264 Accuracy on test set 0.6271611823759063\n",
      "Loss shape: 0.075382947921752935 Accuracy on test set 0.644590072504183\n",
      "Loss shape: 0.190795928239822444 Accuracy on test set 0.6351087562744004\n",
      "Loss shape: 0.145645946264266976 Accuracy on test set 0.6359453430005577\n",
      "Loss shape: 0.11837870627641678 Accuracy on test set 0.6279977691020636\n",
      "Loss shape: 0.097962819039821626 Accuracy on test set 0.6405465699944227\n",
      "Loss shape: 0.120631650090217594 Accuracy on test set 0.6285554935861685\n",
      "Loss shape: 0.152999714016914376 Accuracy on test set 0.6378973786949247\n",
      "Loss shape: 0.14245259761810303 Accuracy on test set 0.6484941438929169\n",
      "Loss shape: 0.07148815691471122 Accuracy on test set 0.6422197434467373\n",
      "Loss shape: 0.096611648797988895 Accuracy on test set 0.6401282766313441\n",
      "Loss shape: 0.09722447395324707 Accuracy on test set 0.6327384272169548\n",
      "Loss shape: 0.058297235518693924 Accuracy on test set 0.6359453430005577\n",
      "Loss shape: 0.079636201262474065 Accuracy on test set 0.6450083658672616\n",
      "Loss shape: 0.117389939725399026 Accuracy on test set 0.6416620189626324\n",
      "Loss shape: 0.123879842460155494 Accuracy on test set 0.6261851645287229\n",
      "Loss shape: 0.097451947629451755 Accuracy on test set 0.6406860011154489\n",
      "Loss shape: 0.183905586600303655 Accuracy on test set 0.6405465699944227\n",
      "Loss shape: 0.08798117190599442 Accuracy on test set 0.6422197434467373\n",
      "Loss shape: 0.135562330484390264 Accuracy on test set 0.6383156720580033\n",
      "Loss shape: 0.095282457768917086 Accuracy on test set 0.6292526491912995\n",
      "Loss shape: 0.083132773637771646 Accuracy on test set 0.6443112102621306\n",
      "Loss shape: 0.080534778535366064 Accuracy on test set 0.6412437255995538\n",
      "Loss shape: 0.117671296000480655 Accuracy on test set 0.638733965421082\n",
      "Loss shape: 0.109329298138618475 Accuracy on test set 0.6423591745677635\n",
      "Loss shape: 0.107896402478218085 Accuracy on test set 0.6436140546569994\n",
      "Loss shape: 0.109758161008358976 Accuracy on test set 0.6380368098159509\n",
      "Loss shape: 0.065196588635444646 Accuracy on test set 0.6369213608477412\n",
      "Loss shape: 0.108144044876098635 Accuracy on test set 0.6303680981595092\n",
      "Loss shape: 0.063368812203407293 Accuracy on test set 0.6337144450641383\n",
      "Loss shape: 0.107033096253871924 Accuracy on test set 0.6401282766313441\n",
      "Loss shape: 0.087108530104160315 Accuracy on test set 0.6399888455103179\n",
      "Loss shape: 0.081522062420845034 Accuracy on test set 0.644032348020078\n",
      "Loss shape: 0.079147003591060644 Accuracy on test set 0.6284160624651423\n",
      "Loss shape: 0.132280796766281134 Accuracy on test set 0.6412437255995538\n",
      "Loss shape: 0.097892597317695624 Accuracy on test set 0.6457055214723927\n",
      "Loss shape: 0.12183242291212082 Accuracy on test set 0.6423591745677635\n",
      "Loss shape: 0.14982381463050842 Accuracy on test set 0.6486335750139431\n",
      "Loss shape: 0.07130934298038483 Accuracy on test set 0.6406860011154489\n",
      "Loss shape: 0.07623407989740372 Accuracy on test set 0.6360847741215839\n",
      "Loss shape: 0.118567854166030885 Accuracy on test set 0.6434746235359732\n",
      "Loss shape: 0.072105981409549715 Accuracy on test set 0.6385945343000557\n",
      "Loss shape: 0.109525039792060855 Accuracy on test set 0.6450083658672616\n",
      "Loss shape: 0.060456182807683945 Accuracy on test set 0.6296709425543782\n",
      "Loss shape: 0.094781734049320224 Accuracy on test set 0.64654210819855\n",
      "Loss shape: 0.0909075066447258 Accuracy on test set 0.6415225878416062\n",
      "Loss shape: 0.07015831023454666 Accuracy on test set 0.6423591745677635\n",
      "Loss shape: 0.10704555362462997 Accuracy on test set 0.6500278862242053\n",
      "Loss shape: 0.14886941015720367 Accuracy on test set 0.6268823201338539\n",
      "Loss shape: 0.08192652463912964 Accuracy on test set 0.6378973786949247\n",
      "Loss shape: 0.1137242391705513 Accuracy on test set 0.6431957612939208\n",
      "Loss shape: 0.04689443111419678 Accuracy on test set 0.6395705521472392\n",
      "Loss shape: 0.08888231217861176 Accuracy on test set 0.6458449525934189\n",
      "Loss shape: 0.03927244991064072 Accuracy on test set 0.6494701617401004\n",
      "Loss shape: 0.088972963392734535 Accuracy on test set 0.6461238148354713\n",
      "Loss shape: 0.045731410384178165 Accuracy on test set 0.6501673173452315\n",
      "Loss shape: 0.121180556714534766 Accuracy on test set 0.6441717791411042\n",
      "Loss shape: 0.159989312291145325 Accuracy on test set 0.6515616285554936\n",
      "Loss shape: 0.102500051259994585 Accuracy on test set 0.6470998326826548\n",
      "Loss shape: 0.10150397568941116 Accuracy on test set 0.645287228109314\n",
      "Loss shape: 0.060031417757272724 Accuracy on test set 0.6418014500836586\n",
      "Loss shape: 0.135882630944252015 Accuracy on test set 0.6494701617401004\n",
      "Loss shape: 0.063591502606868746 Accuracy on test set 0.6411042944785276\n",
      "Loss shape: 0.062289185822010045 Accuracy on test set 0.6436140546569994\n",
      "Loss shape: 0.065816178917884835 Accuracy on test set 0.644032348020078\n",
      "Loss shape: 0.113770499825477664 Accuracy on test set 0.6563022866703848\n",
      "Loss shape: 0.092554278671741494 Accuracy on test set 0.6420803123257111\n",
      "Loss shape: 0.111579827964305884 Accuracy on test set 0.6497490239821528\n",
      "Loss shape: 0.059336241334676746"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    it = 1 # we chose 1 and not 0 because taht can create problem with the modulo\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Be sure to start the loop with zeros grad\n",
    "        if weights.grad is not None:\n",
    "            weights.grad.zero_()\n",
    "        \n",
    "        # data = data.view((-1, 28*28)) already done in the dataset\n",
    "        outputs = torch.matmul(data, weights)\n",
    "\n",
    "        log_softmax = F.log_softmax(outputs, dim=1)   # log has better convergence properties\n",
    "        \n",
    "        loss = F.nll_loss(log_softmax, targets)    # nll_loss because we want to minimize the negative log likelihood (Gradient descent)\n",
    "        \n",
    "        print(\"\\rLoss shape: {}\".format(loss), end=\"\")\n",
    "        \n",
    "        # Compute the gradients for each variables\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= 0.1*weights.grad\n",
    "            \n",
    "        it += 1\n",
    "        if it % 100 == 0:\n",
    "            test(weights, test_loader)\n",
    "            \n",
    "        if it > 5000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_idx, (data, target) = next(enumerate(test_loader))\n",
    "#data = data.view((-1, 28*28))\n",
    "\n",
    "outputs = torch.matmul(data, weights)\n",
    "softmax = F.softmax(outputs, dim=1)\n",
    "pred = softmax.argmax(dim=1, keepdim=True)\n",
    "\n",
    "plt.imshow(data[1].view(28, 28), cmap=\"gray\") # *255 isn't usefull beacause of cmap=\"gray\"\n",
    "plt.title(\"Predicted class {}\".format(pred[1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
